



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="http://autokeras.com/start/">
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.0">
    
    
      
        <title>Getting Started - Auto-Keras</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.0284f74d.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.01803549.css">
      
      
        
        
        <meta name="theme-color" content="#009688">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-44322747-3", "autokeras.com")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="teal" data-md-color-accent="teal">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#getting-started" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="http://autokeras.com" title="Auto-Keras" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Auto-Keras
            </span>
            <span class="md-header-nav__topic">
              
                Getting Started
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/keras-team/autokeras" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GitHub
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="http://autokeras.com" title="Auto-Keras" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Auto-Keras
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/keras-team/autokeras" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Getting Started
      </label>
    
    <a href="./" title="Getting Started" class="md-nav__link md-nav__link--active">
      Getting Started
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" title="Installation" class="md-nav__link">
    Installation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#latest-stable-version-pip-installation" title="Latest Stable Version (pip installation):" class="md-nav__link">
    Latest Stable Version (pip installation):
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bleeding-edge-version-manual-installation" title="Bleeding Edge Version (manual installation):" class="md-nav__link">
    Bleeding Edge Version (manual installation):
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-simple-example" title="A Simple Example" class="md-nav__link">
    A Simple Example
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-with-numpy-array-npy-format" title="Data with numpy array (.npy) format." class="md-nav__link">
    Data with numpy array (.npy) format.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-if-your-data-are-raw-image-files-eg-jpg-png-bmp" title="What if your data are raw image files (e.g. .jpg, .png, .bmp)?" class="md-nav__link">
    What if your data are raw image files (e.g. .jpg, .png, .bmp)?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#enable-multi-gpu-training" title="Enable Multi-GPU Training" class="md-nav__link">
    Enable Multi-GPU Training
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#portable-models" title="Portable Models" class="md-nav__link">
    Portable Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-export-portable-model" title="How to export Portable model?" class="md-nav__link">
    How to export Portable model?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-to-load-exported-portable-model" title="How to load exported Portable model?" class="md-nav__link">
    How to load exported Portable model?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-visualizations" title="Model Visualizations" class="md-nav__link">
    Model Visualizations
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-visualize-the-best-selected-architecture" title="How to visualize the best selected architecture?" class="md-nav__link">
    How to visualize the best selected architecture?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#net-modules" title="Net Modules" class="md-nav__link">
    Net Modules
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mlpmodule-tutorial" title="MlpModule tutorial." class="md-nav__link">
    MlpModule tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnnmodule-tutorial" title="CnnModule tutorial." class="md-nav__link">
    CnnModule tutorial.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#task-modules" title="Task Modules" class="md-nav__link">
    Task Modules
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automated-text-classifier-tutorial" title="Automated text classifier tutorial." class="md-nav__link">
    Automated text classifier tutorial.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pretrained-models" title="Pretrained Models" class="md-nav__link">
    Pretrained Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#object-detection-tutorial" title="Object detection tutorial." class="md-nav__link">
    Object detection tutorial.
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#by-wuyang-chen-from-dr-atlas-wangs-group-at-cse-department-texas-am" title="by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&amp;M." class="md-nav__link">
    by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&amp;M.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentiment-analysis-tutorial" title="Sentiment Analysis tutorial." class="md-nav__link">
    Sentiment Analysis tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#topic-classification-tutorial" title="Topic Classification tutorial." class="md-nav__link">
    Topic Classification tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#voice-generator-tutorial" title="Voice generator tutorial." class="md-nav__link">
    Voice generator tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#voice-recognizer-tutorial" title="Voice recognizer tutorial." class="md-nav__link">
    Voice recognizer tutorial.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../docker/" title="Docker" class="md-nav__link">
      Docker
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../temp/contribute/" title="Contributing Guide" class="md-nav__link">
      Contributing Guide
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../nas/" title="Neural Architecture Search" class="md-nav__link">
      Neural Architecture Search
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Documentation
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        Documentation
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/supervised.md" title="supervised" class="md-nav__link">
      supervised
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/image_supervised.md" title="image_supervised" class="md-nav__link">
      image_supervised
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/bayesian.md" title="bayesian" class="md-nav__link">
      bayesian
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/search.md" title="search" class="md-nav__link">
      search
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/graph.md" title="graph" class="md-nav__link">
      graph
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/preprocessor/" title="preprocessor" class="md-nav__link">
      preprocessor
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/model_trainer.md" title="model_trainer" class="md-nav__link">
      model_trainer
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/utils.md" title="utils" class="md-nav__link">
      utils
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../temp/generator.md" title="generator" class="md-nav__link">
      generator
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../about/" title="About" class="md-nav__link">
      About
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" title="Installation" class="md-nav__link">
    Installation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#latest-stable-version-pip-installation" title="Latest Stable Version (pip installation):" class="md-nav__link">
    Latest Stable Version (pip installation):
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bleeding-edge-version-manual-installation" title="Bleeding Edge Version (manual installation):" class="md-nav__link">
    Bleeding Edge Version (manual installation):
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-simple-example" title="A Simple Example" class="md-nav__link">
    A Simple Example
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-with-numpy-array-npy-format" title="Data with numpy array (.npy) format." class="md-nav__link">
    Data with numpy array (.npy) format.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-if-your-data-are-raw-image-files-eg-jpg-png-bmp" title="What if your data are raw image files (e.g. .jpg, .png, .bmp)?" class="md-nav__link">
    What if your data are raw image files (e.g. .jpg, .png, .bmp)?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#enable-multi-gpu-training" title="Enable Multi-GPU Training" class="md-nav__link">
    Enable Multi-GPU Training
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#portable-models" title="Portable Models" class="md-nav__link">
    Portable Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-export-portable-model" title="How to export Portable model?" class="md-nav__link">
    How to export Portable model?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-to-load-exported-portable-model" title="How to load exported Portable model?" class="md-nav__link">
    How to load exported Portable model?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-visualizations" title="Model Visualizations" class="md-nav__link">
    Model Visualizations
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-to-visualize-the-best-selected-architecture" title="How to visualize the best selected architecture?" class="md-nav__link">
    How to visualize the best selected architecture?
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#net-modules" title="Net Modules" class="md-nav__link">
    Net Modules
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mlpmodule-tutorial" title="MlpModule tutorial." class="md-nav__link">
    MlpModule tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnnmodule-tutorial" title="CnnModule tutorial." class="md-nav__link">
    CnnModule tutorial.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#task-modules" title="Task Modules" class="md-nav__link">
    Task Modules
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automated-text-classifier-tutorial" title="Automated text classifier tutorial." class="md-nav__link">
    Automated text classifier tutorial.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pretrained-models" title="Pretrained Models" class="md-nav__link">
    Pretrained Models
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#object-detection-tutorial" title="Object detection tutorial." class="md-nav__link">
    Object detection tutorial.
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#by-wuyang-chen-from-dr-atlas-wangs-group-at-cse-department-texas-am" title="by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&amp;M." class="md-nav__link">
    by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&amp;M.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentiment-analysis-tutorial" title="Sentiment Analysis tutorial." class="md-nav__link">
    Sentiment Analysis tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#topic-classification-tutorial" title="Topic Classification tutorial." class="md-nav__link">
    Topic Classification tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#voice-generator-tutorial" title="Voice generator tutorial." class="md-nav__link">
    Voice generator tutorial.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#voice-recognizer-tutorial" title="Voice recognizer tutorial." class="md-nav__link">
    Voice recognizer tutorial.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="getting-started">Getting Started</h1>
<hr />
<h2 id="installation">Installation</h2>
<p>The installation of Auto-Keras is the same as other python packages. </p>
<p><strong>Note:</strong> currently, Auto-Keras is only compatible with: <strong>Python 3.6</strong>.</p>
<h3 id="latest-stable-version-pip-installation">Latest Stable Version (<code>pip</code> installation):</h3>
<p>You can run the following <code>pip</code> installation command in your terminal to install the latest stable version.</p>
<pre><code>pip install autokeras
</code></pre>
<h3 id="bleeding-edge-version-manual-installation">Bleeding Edge Version (manual installation):</h3>
<p>If you want to install the latest development version. 
You need to download the code from the GitHub repo and run the following commands in the project directory.</p>
<pre><code>pip install -r requirements.txt
python setup.py install
</code></pre>
<h2 id="a-simple-example">A Simple Example</h2>
<p>We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs. </p>
<h3 id="data-with-numpy-array-npy-format">Data with numpy array (.npy) format.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/a_simple_example/mnist.py">[source]</a></p>
<p>If the images and the labels are already formatted into numpy arrays, you can </p>
<pre><code>from keras.datasets import mnist
from autokeras.image.image_supervised import ImageClassifier

if __name__ == '__main__':
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(x_train.shape + (1,))
    x_test = x_test.reshape(x_test.shape + (1,))

    clf = ImageClassifier(verbose=True)
    clf.fit(x_train, y_train, time_limit=12 * 60 * 60)
    clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)
    y = clf.evaluate(x_test, y_test)
    print(y)
</code></pre>
<p>In the example above, the images and the labels are already formatted into numpy arrays.</p>
<h3 id="what-if-your-data-are-raw-image-files-eg-jpg-png-bmp">What if your data are raw image files (<em>e.g.</em> .jpg, .png, .bmp)?</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/a_simple_example/load_raw_image.py">[source]</a></p>
<p>You can use our <code>load_image_dataset</code> function to load the images and their labels as follows.</p>
<pre><code>from autokeras.image.image_supervised import load_image_dataset

x_train, y_train = load_image_dataset(csv_file_path="train/label.csv",
                                      images_path="train")
print(x_train.shape)
print(y_train.shape)

x_test, y_test = load_image_dataset(csv_file_path="test/label.csv",
                                    images_path="test")
print(x_test.shape)
print(y_test.shape)
</code></pre>
<p>The argument <code>csv_file_path</code> is the path to the CSV file containing the image file names and their corresponding labels. Both csv files and the raw image datasets could be downloaded from <a href="https://drive.google.com/a/tamu.edu/file/d/10TyvztrdL0fBaFlgGaqoRTlS3ts4faM8/view?usp=sharing">link</a>.
Here is an example of the csv file.</p>
<pre><code>File Name,Label
00000.jpg,5
00001.jpg,0
00002.jpg,4
00003.jpg,1
00004.jpg,9
00005.jpg,2
00006.jpg,1
...
</code></pre>
<p>The second argument <code>images_path</code> is the path to the directory containing all the images with those file names listed in the CSV file.
The returned values <code>x_train</code> and <code>y_train</code> are the numpy arrays,
which can be directly feed into the <code>fit</code> function of <code>ImageClassifier</code>.</p>
<p>This CSV file for train or test can be created from folders containing images of a specific class (meaning label):</p>
<pre><code>train
└───class_1
│   │   class_1_image_1.png
│   │   class_1_image_2.png
|   |   ...
└───class_2
    │   class_2_image_1.png
    │   class_2_image_2.png
    |   ...
</code></pre>

<p>The code below shows an example of how to create the CSV:</p>
<pre><code>train_dir = 'train' # Path to the train directory
class_dirs = [i for i in os.listdir(path=train_dir) if os.path.isdir(os.path.join(train_dir, i))]
 with open('train/label.csv', 'w') as train_csv:
    fieldnames = ['File Name', 'Label']
    writer = csv.DictWriter(train_csv, fieldnames=fieldnames)
    writer.writeheader()
    label = 0
    for current_class in class_dirs:
        for image in os.listdir(os.path.join(train_dir, current_class)):
            writer.writerow({'File Name': str(image), 'Label':label})
        label += 1
    train_csv.close()
</code></pre>

<h3 id="enable-multi-gpu-training">Enable Multi-GPU Training</h3>
<p>Auto-Keras support multiple GPU training in the default setting. 
There's no additional step needed to enable multiple GPU training. 
However, if multiple-GPU training is not a desirable behavior. 
You can disable it via environmental variable. <code>CUDA_VISIBLE_DEVICES</code>. 
For example, in your bash: <code>export CUDA_VISIBLE_DEVICES=0</code>. 
Keep in mind that when using multiple-GPU, make sure batch size is big enough that multiple-gpu context switch overhead won't effect the performance too much. 
Otherwise multiple-gpu training may be slower than single-GPU training.</p>
<h2 id="portable-models">Portable Models</h2>
<h3 id="how-to-export-portable-model">How to export Portable model?</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/portable_models/portable_load.py">[source]</a></p>
<pre><code>from autokeras import ImageClassifier
clf = ImageClassifier(verbose=True, augment=False)
clf.export_autokeras_model(model_file_name)
</code></pre>
<p>The model will be stored into the path <code>model_file_name</code>. </p>
<h3 id="how-to-load-exported-portable-model">How to load exported Portable model?</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/portable_models/portable_load.py">[source]</a></p>
<pre><code>from autokeras.utils import pickle_from_file
model = pickle_from_file(model_file_name)
results = model.evaluate(x_test, y_test)
print(results)
</code></pre>
<p>The model will be loaded from the path <code>model_file_name</code> and then you can use the functions listed in <code>PortableImageSupervised</code>.</p>
<h2 id="model-visualizations">Model Visualizations</h2>
<h3 id="how-to-visualize-the-best-selected-architecture">How to visualize the best selected architecture?</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/visualizations/visualize.py">[source]</a></p>
<p>While trying to create a model, let's say an Image classifier on MNIST, there is a facility for the user to visualize a .PDF depiction of the best architecture that was chosen by autokeras, after model training is complete. </p>
<p>Prerequisites : 
1) graphviz must be installed in your system. Refer <a href="https://graphviz.gitlab.io/download/">Installation Guide</a><br />
2) Additionally, also install "graphviz" python package using pip / conda</p>
<pre><code>pip:  pip install graphviz

conda : conda install -c conda-forge python-graphviz
</code></pre>
<p>If the above installations are complete, proceed with the following steps :</p>
<p>Step 1 : Specify a <em>path</em> before starting your model training</p>
<pre><code>clf = ImageClassifier(path="~/automodels/",verbose=True, augment=False) # Give a custom path of your choice
clf.fit(x_train, y_train, time_limit=30 * 60)
clf.final_fit(x_train, y_train, x_test, y_test, retrain=True)
</code></pre>
<p>Step 2 : After the model training is complete, run <em>examples/visualize.py</em>, whilst passing the same <em>path</em> as parameter</p>
<pre><code>if __name__ == '__main__':
    visualize('~/automodels/')
</code></pre>
<h2 id="net-modules">Net Modules</h2>
<h3 id="mlpmodule-tutorial">MlpModule tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/net_modules/mlp_module.py">[source]</a></p>
<p><code>MlpGenerator</code> in <code>net_module.py</code> is a child class of <code>Networkmodule</code>. It can generates neural architecture with MLP modules </p>
<p>Normally, there's two place to call the MlpGenerator, one is call <code>MlpGenerator.fit</code> while the other is <code>MlpGenerator.final_fit</code>.</p>
<p>For example, in a image classification class <code>ImageClassifier</code>, one can initialize the cnn module as:</p>
<pre><code class="python">mlpModule = MlpModule(loss, metric, searcher_args, path, verbose)
</code></pre>

<p>Where:
* <code>loss</code> and <code>metric</code> determines by the type of training model(classification or regression or others)
* <code>search_args</code> can be referred in <code>search.py</code>
* <code>path</code> is the path to store the whole searching process and generated model.
* <code>verbose</code> is a boolean. Setting it to true prints to stdout.</p>
<p>Then, for the searching part, one can call:</p>
<pre><code class="python">mlpModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60)
</code></pre>

<p>where:
* n_output_node: A integer value represent the number of output node in the final layer.
* input_shape: A tuple to express the shape of every train entry. For example,
                MNIST dataset would be (28,28,1).
* train_data: A PyTorch DataLoader instance representing the training data.
* test_data: A PyTorch DataLoader instance representing the testing data.
* time_limit: A integer value represents the time limit on searching for models.</p>
<p>And for final testing(testing the best searched model), one can call:</p>
<pre><code class="python">mlpModule.final_fit(train_data, test_data, trainer_args=None, retrain=False)
</code></pre>

<p>where:
* train_data: A DataLoader instance representing the training data.
* test_data: A DataLoader instance representing the testing data.
* trainer_args: A dictionary containing the parameters of the ModelTrainer constructor.
* retrain: A boolean of whether reinitialize the weights of the model.</p>
<h3 id="cnnmodule-tutorial">CnnModule tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/net_modules/cnn_module.py">[source]</a></p>
<p><code>CnnGenerator</code> in <code>net_module.py</code> is a child class of <code>Networkmodule</code>. It can generates neural architecture with basic cnn modules
and the ResNet module. </p>
<p>Normally, there's two place to call the CnnGenerator, one is call <code>CnnGenerator.fit</code> while the other is <code>CnnGenerator.final_fit</code>.</p>
<p>For example, in a image classification class <code>ImageClassifier</code>, one can initialize the cnn module as:</p>
<pre><code class="python">from autokeras import CnnModule
from autokeras.nn.loss_function import classification_loss
from autokeras.nn.metric import Accuracy

TEST_FOLDER = &quot;test&quot;
cnnModule = CnnModule(loss=classification_loss, metric=Accuracy, searcher_args={}, path=TEST_FOLDER, verbose=False)
</code></pre>

<p>Where:
* <code>loss</code> and <code>metric</code> determines by the type of training model(classification or regression or others)
* <code>search_args</code> can be referred in <code>search.py</code>
* <code>path</code> is the path to store the whole searching process and generated model.
* <code>verbose</code> is a boolean. Setting it to true prints to stdout.</p>
<p>Then, for the searching part, one can call:</p>
<pre><code class="python">cnnModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60)
</code></pre>

<p>where:
* n_output_node: A integer value represent the number of output node in the final layer.
* input_shape: A tuple to express the shape of every train entry. For example,
                MNIST dataset would be (28,28,1).
* train_data: A PyTorch DataLoader instance representing the training data.
* test_data: A PyTorch DataLoader instance representing the testing data.
* time_limit: A integer value represents the time limit on searching for models.</p>
<p>And for final testing(testing the best searched model), one can call:</p>
<pre><code class="python">cnnModule.final_fit(train_data, test_data, trainer_args=None, retrain=False)
</code></pre>

<p>where:
* train_data: A DataLoader instance representing the training data.
* test_data: A DataLoader instance representing the testing data.
* trainer_args: A dictionary containing the parameters of the ModelTrainer constructor.
* retrain: A boolean of whether reinitialize the weights of the model.</p>
<h2 id="task-modules">Task Modules</h2>
<h3 id="automated-text-classifier-tutorial">Automated text classifier tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/task_modules/text/text_classification.py">[source]</a></p>
<p>Class <code>TextClassifier</code> and <code>TextRegressor</code> are designed for automated generate best performance cnn neural architecture
for a given text dataset. </p>
<pre><code class="python">    clf = TextClassifier(verbose=True)
    clf.fit(x=x_train, y=y_train, time_limit=12 * 60 * 60)
</code></pre>

<ul>
<li>x_train: string format text data</li>
<li>y_train: int format text label</li>
</ul>
<p>After searching the best model, one can call <code>clf.final_fit</code> to test the best model found in searching.</p>
<p><strong>Notes:</strong> Preprocessing of the text data:
* Class <code>TextClassifier</code> and <code>TextRegressor</code> contains a pre-process of the text data. Which means the input data
should be in string format. 
* The default pre-process model uses the <a href="https://nlp.stanford.edu/projects/glove/">glove6B model</a> from Stanford NLP. 
* To change the default setting of the pre-process model, one need to change the corresponding variable:
<code>EMBEDDING_DIM</code>, <code>PRE_TRAIN_FILE_LINK</code>, <code>PRE_TRAIN_FILE_LINK</code>, <code>PRE_TRAIN_FILE_NAME</code> in <code>constant.py</code>.</p>
<h2 id="pretrained-models">Pretrained Models</h2>
<h3 id="object-detection-tutorial">Object detection tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/examples/pretrained_models/object_detection/object_detection_example.py">[source]</a></p>
<h4 id="by-wuyang-chen-from-dr-atlas-wangs-group-at-cse-department-texas-am">by Wuyang Chen from <a href="http://www.atlaswang.com/">Dr. Atlas Wang's group</a> at CSE Department, Texas A&amp;M.</h4>
<p>class_id_mapping = {0 : "Business", 1 : "Sci/Tech", 2 : "Sports", 3 : "World"}</p>
<p><code>ObjectDetector</code> in <code>object_detector.py</code> is a child class of <code>Pretrained</code>. Currently it can load a pretrained SSD model (<a href="https://arxiv.org/abs/1512.02325">Liu, Wei, et al. "Ssd: Single shot multibox detector." European conference on computer vision. Springer, Cham, 2016.</a>) and find object(s) in a given image.</p>
<p>Let's first import the ObjectDetector and create a detection model (<code>detector</code>) with</p>
<pre><code class="python">from autokeras.pretrained.object_detector import ObjectDetector
detector = ObjectDetector()
</code></pre>

<p>It will automatically download and load the weights into <code>detector</code>.</p>
<p><strong>Note:</strong>  the <code>ObjectDetector</code> class can automatically detect the existance of available cuda device(s), and use the device if exists.</p>
<p>Finally you can make predictions against an image:</p>
<pre><code class="python">    results = detector.predict(&quot;/path/to/images/000001.jpg&quot;, output_file_path=&quot;/path/to/images/&quot;)
</code></pre>

<p>Function <code>detector.predict()</code> requires the path to the image. If the <code>output_file_path</code> is not given, the <code>detector</code> will just return the numerical results as a list of dictionaries. Each dictionary is like {"left": int, "top": int, "width": int, "height": int: "category": str, "confidence": float}, where <code>left</code> and <code>top</code> is the (left, top) coordinates of the bounding box of the object and <code>width</code> and <code>height</code> are width and height of the box. <code>category</code> is a string representing the class the object belongs to, and the confidence can be regarded as the probability that the model believes its prediction is correct. If the <code>output_file_path</code> is given, then the results mentioned above will be plotted and saved in a new image file with suffix "_prediction" into the given <code>output_file_path</code>. If you run the example/object_detection/object_detection_example.py, you will get result
<code>[{'category': 'person', 'width': 331, 'height': 500, 'left': 17, 'confidence': 0.9741123914718628, 'top': 0}]</code></p>
<h3 id="sentiment-analysis-tutorial">Sentiment Analysis tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/autokeras/pretrained/text_classifier.py">[source]</a></p>
<p>The sentiment analysis module provides an interface to find the sentiment of any text. The pretrained model is obtained by training <a href="https://arxiv.org/abs/1810.04805">Google AI’s BERT model</a> on <a href="http://ai.stanford.edu/~amaas/data/sentiment/">IMDb dataset</a>. </p>
<p>Let’s import the <code>SentimentAnalysis</code> module from <em>text_classifier.py</em>. It is derived from the super class <code>TextClassifier</code> which is the child class of <code>Pretrained</code> class.</p>
<pre><code class="python">from autokeras.pretrained.text_classifier import SentimentAnalysis
sentiment_analysis = SentimentAnalysis()
</code></pre>

<p>During initialization of <code>SentimentAnalysis</code>, the pretrained model is loaded into memory i.e. CPU’s or GPU’s, if available.</p>
<p>Now, you may directly call the <code>predict</code> function in <code>SentimentAnalysis</code> class on any input sentence provided as a string as shown below. The function returns a value between 0 and 1. </p>
<pre><code class="python">polarity = sentiment_cls.predict(&quot;The model is working well..&quot;)
</code></pre>

<p><strong>Note:</strong> If the output value of the <code>predict</code> function is close to 0, it implies the statement has negative sentiment, whereas value close to 1 implies positive sentiment.</p>
<p>If you run <em>sentiment_analysis_example.py</em>, you should get an output value of 0.9 which implies that the input statement <em>The model is working well..</em> has strong positive sentiment.</p>
<h3 id="topic-classification-tutorial">Topic Classification tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/autokeras/pretrained/text_classifier.py">[source]</a></p>
<p>The topic classifier module provides an interface to find the topic of any text. The pretrained model is obtained by training <a href="https://arxiv.org/abs/1810.04805">Google AI’s BERT model</a> on <a href="https://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html">AGNews dataset</a>. </p>
<p>Let’s import the <code>TopicClassifier</code> module from <em>text_classifier.py</em>. It is derived from the super class <code>TextClassifier</code> which is the child class of <code>Pretrained</code> class. </p>
<pre><code class="python">from autokeras.pretrained.text_classifier import TopicClassifier
topic_classifier = TopicClassifier()
</code></pre>

<p>During initialization of <code>TopicClassifier</code>, the pretrained model is loaded into memory i.e. CPU’s or GPU’s, if available.</p>
<p>Now, you may directly call the <code>predict</code> function in <code>TopicClassifier</code> class on any input sentence provided as a string as shown below. The function returns one of the fours topics <strong>Business</strong>, <strong>Sci/Tech</strong>, <strong>World</strong> and <strong>Sports</strong>. </p>
<pre><code class="python">class_name = topic_classifier.predict(&quot;With some more practice, they will definitely make it to finals..&quot;)
</code></pre>

<p>If you run <em>topic_classifier_example.py</em>, you should see the predict function returns the label <strong>Sports</strong>, which is the predicted label for the input statement.</p>
<h3 id="voice-generator-tutorial">Voice generator tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/autokeras/pretrained/voice_generator/voice_generator.py">[source]</a></p>
<p>The voice generator is a refactor of <a href="https://github.com/r9y9/deepvoice3_pytorch">deepvoice3</a>. 
The structure contains three main parts:</p>
<ul>
<li><strong>Encoder</strong>: A  fully-convolutional  encoder,  which  converts  textual  features  to  an  internallearned representation.</li>
<li><strong>Decoder</strong>: A fully-convolutional causal decoder, which decodes the learned representationwith a multi-hop convolutional attention mechanism into a low-dimensional audio repre-sentation (mel-scale spectrograms) in an autoregressive manner.</li>
<li><strong>Converter</strong>:  A fully-convolutional post-processing network, which predicts final vocoderparameters (depending on the vocoder choice) from the decoder hidden states.  Unlike thedecoder, the converter is non-causal and can thus depend on future context information</li>
</ul>
<p>For more details, please refer the original paper: 
<a href="https://arxiv.org/pdf/1710.07654.pdf"><strong>Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning</strong></a></p>
<p>Example:</p>
<pre><code class="python">from autokeras.pretrained import VoiceGenerator
voice_generator = VoiceGenerator()
text = &quot;The approximation of pi is 3.14&quot;
voice_generator.predict(text, &quot;test.wav&quot;)
</code></pre>

<h3 id="voice-recognizer-tutorial">Voice recognizer tutorial.</h3>
<p><a href="https://github.com/keras-team/autokeras/blob/master/autokeras/pretrained/voice_recognizer.py">[source]</a></p>
<p>The voice recognizer is a refactor of <a href="https://github.com/SeanNaren/deepspeech.pytorch">deepspeech</a>. 
The model structure contains two parts:
* Encoder: Convolutional layer followed by recurrent neural network and then fully convert network. Output is the hidden voice information.
* Decoder: Decode the hidden voice information to the voice wave.</p>
<p>For more details, please refer the original paper:
<a href="https://arxiv.org/abs/1512.02595"><strong>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</strong></a></p>
<p>Because currently <a href="https://github.com/pytorch/audio">torchaudio</a> does not support pip install. So the current package doesn't support audio parsing part.
To use the voice recognizer, one should first parse the audio following the standard below:</p>
<ul>
<li>First, install the <a href="https://github.com/pytorch/audio">torchaudio</a>, the install process can refer the repo.</li>
<li>Seconder use the following audio parser</li>
</ul>
<pre><code class="python">from autokeras.constant import Constant
import torchaudio
import scipy.signal
import librosa
import torch
import numpy as np

def load_audio(path):
    sound, _ = torchaudio.load(path)
    sound = sound.numpy()
    if len(sound.shape) &gt; 1:
        if sound.shape[0] == 1:
            sound = sound.squeeze()
        else:
            sound = sound.mean(axis=0)  # multiple channels, average
    return sound


class SpectrogramParser:
    def __init__(self, audio_conf, normalize=False, augment=False):
        &quot;&quot;&quot;
        Parses audio file into spectrogram with optional normalization and various augmentations
        :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds
        :param normalize(default False):  Apply standard mean and deviation normalization to audio tensor
        :param augment(default False):  Apply random tempo and gain perturbations
        &quot;&quot;&quot;
        super(SpectrogramParser, self).__init__()
        self.window_stride = audio_conf['window_stride']
        self.window_size = audio_conf['window_size']
        self.sample_rate = audio_conf['sample_rate']
        self.window = scipy.signal.hamming
        self.normalize = normalize
        self.augment = augment
        self.noise_prob = audio_conf.get('noise_prob')

    def parse_audio(self, audio_path):
        y = load_audio(audio_path)

        n_fft = int(self.sample_rate * self.window_size)
        win_length = n_fft
        hop_length = int(self.sample_rate * self.window_stride)
        # STFT
        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,
                         win_length=win_length, window=self.window)
        spect, _ = librosa.magphase(D)
        # S = log(S+1)
        spect = np.log1p(spect)
        spect = torch.FloatTensor(spect)
        if self.normalize:
            mean = spect.mean()
            std = spect.std()
            spect.add_(-mean)
            spect.div_(std)

        return spect

parser = SpectrogramParser(Constant.VOICE_RECONGINIZER_AUDIO_CONF, normalize=True)
spect = parser.parse_audio(&quot;test.wav&quot;).contiguous()
</code></pre>

<p>After this we will have the audio parsed as torch tensor in variable <code>spect</code>. Then we can use the following to recognize the voice:</p>
<pre><code class="python">from autokeras.pretrained import VoiceRecognizer
voice_recognizer = VoiceRecognizer()
print(voice_recognizer.predict(audio_data=spect))
</code></pre>

<p>This voice recognizer pretrained model is well tuned based on the <a href="http://www.speech.cs.cmu.edu/databases/an4/">AN4</a> dataset. It has a large probability 
cannot perform well on other dataset. </p>
<!-- [Data with numpy array (.npy) format.]: https://github.com/keras-team/autokeras/blob/master/examples/a_simple_example/mnist.py
[What if your data are raw image files (*e.g.* .jpg, .png, .bmp)?]: https://github.com/keras-team/autokeras/blob/master/examples/a_simple_example/load_raw_image.py
[How to export Portable model]: https://github.com/keras-team/autokeras/blob/master/examples/portable_models/portable_load.py
[How to load exported Portable model?]: https://github.com/keras-team/autokeras/blob/master/examples/portable_models/portable_load.py
[How to visualize the best selected architecture?]: https://github.com/keras-team/autokeras/blob/master/examples/visualizations/visualize.py
[MlpModule tutorial]: https://github.com/keras-team/autokeras/blob/master/examples/net_modules/mlp_module.py
[CnnModule tutorial]: https://github.com/keras-team/autokeras/blob/master/examples/net_modules/cnn_module.py
[Automated text classifier tutorial]: https://github.com/keras-team/autokeras/blob/master/examples/task_modules/text/text.py
[Object Detection tutorial]: https://github.com/keras-team/autokeras/blob/master/examples/pretrained_models/object_detection/object_detection_example.py -->
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href=".." title="Home" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Home
              </span>
            </div>
          </a>
        
        
          <a href="../docker/" title="Docker" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Docker
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.245445c6.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>